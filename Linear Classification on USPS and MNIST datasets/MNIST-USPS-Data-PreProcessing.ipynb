{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from sklearn.cluster import KMeans\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data: 5\n",
      "validation_data: 3\n",
      "test_data: 7\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "filename = '/Users/ektakatiyar/Downloads/ML/Project3/mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "print(\"training_data: \"+ str(training_data[1][0]))\n",
    "print(\"validation_data: \"+ str(validation_data[1][0]))\n",
    "print(\"test_data: \"+ str(test_data[1][0]))\n",
    "print(test_data[0].shape)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "majorityListMNIST= []\n",
    "majorityListUSPS=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "training_label = np.zeros((50000, 10))\n",
    "training_label[np.arange(50000), training_data[1]] = 1\n",
    "print(training_label[0])\n",
    "print(training_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "validation_label=np.zeros((10000, 10))\n",
    "validation_label[np.arange(10000), validation_data[1]] = 1\n",
    "print(validation_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "test_label = np.zeros((10000,10))\n",
    "test_label[np.arange(10000),test_data[1]]=1\n",
    "print(test_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load USPS on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPSMat: 19999\n",
      "USPSTar: 19999\n",
      "savedImg: <PIL.Image.Image image mode=P size=28x28 at 0x10AA6A470>\n"
     ]
    }
   ],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = '/Users/ektakatiyar/Downloads/ML/Project3/USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)\n",
    "print(\"USPSMat: \"+str(len(USPSMat)))\n",
    "print(\"USPSTar: \"+str(len(USPSTar)))\n",
    "print(\"savedImg: \"+str(savedImg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression Model-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features, weights):\n",
    "    print(\"Feature shape :\"+ str(features.shape))\n",
    "    #print(\"Weights shape :\"+ str(weights.shape))\n",
    "    x = np.dot(features, weights)\n",
    "    return softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ex = np.exp(x)\n",
    "    sum_ex = np.sum( np.exp(x))\n",
    "    return ex/sum_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(features, labels, weights):\n",
    "    '''\n",
    "    Using Mean Absolute Error\n",
    "\n",
    "    Features:(100,3)\n",
    "    Labels: (100,1)\n",
    "    Weights:(3,1)\n",
    "    Returns 1D matrix of predictions\n",
    "    Cost = ( log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)\n",
    "    '''\n",
    "    observations = len(labels)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #Take the error when label=1\n",
    "    class1_cost = -labels*np.log(predictions)\n",
    "\n",
    "    #Take the error when label=0\n",
    "    class2_cost = (1-labels)*np.log(1-predictions)\n",
    "\n",
    "    #Take the sum of both costs\n",
    "    cost = class1_cost - class2_cost\n",
    "\n",
    "    #Take the average cost\n",
    "    cost = cost.sum()/observations\n",
    "    print(\"Cost: \"+str(cost))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(features, labels, weights, lr):\n",
    "    #print(\"weights:\"+ str(weights.shape))\n",
    "    N = len(features)\n",
    "\n",
    "    #1 - Get Predictions\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #2 Transpose features from (200, 3) to (3, 200)\n",
    "    # So we can multiply w the (200,1)  cost matrix.\n",
    "    # Returns a (3,1) matrix holding 3 partial derivatives --\n",
    "    # one for each feature -- representing the aggregate\n",
    "    # slope of the cost function across all observations\n",
    "    gradient = np.dot(features.T,  predictions - labels)\n",
    "\n",
    "    #3 Take the average cost derivative for each feature\n",
    "    gradient /= N\n",
    "\n",
    "    #4 - Multiply the gradient by our learning rate\n",
    "    gradient *= lr\n",
    "\n",
    "    #5 - Subtract from our weights to minimize cost\n",
    "    weights -= gradient\n",
    "    #print(\"weights :\"+ str(weights))\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels, weights, lr, iters):\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        weights = update_weights(features, labels, weights, lr)\n",
    "        print(weights.shape)\n",
    "        #Calculate error for auditing purposes\n",
    "        cost = cost_function(features, labels, weights)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Log Progress\n",
    "        if i % 1000 == 0:\n",
    "            print (\"iter: \"+str(i) + \" cost: \"+str(cost))\n",
    "\n",
    "    return weights, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_model(predicted_labels, actual_labels):\n",
    "    count=0\n",
    "    #print(\"predicted_labels :\"+ str(predicted_labels.shape))\n",
    "    #print(\"actual_labels :\"+ str(actual_labels.shape))\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if np.argmax(predicted_labels[i])==np.argmax(actual_labels[i]):\n",
    "            count=count+1;\n",
    "    accuracy= (count/len(predicted_labels))*100\n",
    "    print(\"Accuracy: \"+ str(accuracy))\n",
    "    LOG1_pred=np.array([np.argmax(t) for t in predicted_labels])\n",
    "    \n",
    "    print(\"Classification report: \\n\"+ str(classification_report(test_data[1], LOG1_pred)))\n",
    "    print(\"Confusion matrix : \\n\"+ str(confusion_matrix(test_data[1], LOG1_pred)))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.121317792034866\n",
      "iter: 0 cost: 13.121317792034866\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.120255772394918\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.119195319935988\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.11813643609406\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.117079122307548\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.116023380017303\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.11496921066667\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.113916615701429\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.112865596569849\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.11181615472269\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.110768291613178\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.109722008697053\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.108677307432538\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.107634189280395\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.10659265570386\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.105552708168736\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.104514348143312\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.103477577098449\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.102442396507543\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.10140880784652\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.100376812593897\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.099346412230725\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.09831760824064\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.097290402109872\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.096264795327212\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.095240789384064\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.094218385774415\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.093197585994881\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.092178391544675\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.09116080392564\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.090144824642252\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.089130455201618\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.088117697113496\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.087106551890301\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.08609702104707\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.08508910610157\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.084082808574196\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.083078129988028\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.082075071868857\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.081073635745152\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.080073823148084\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.079075635611565\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.078079074672187\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.077084141869298\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.076090838744962\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.075099166844\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.074109127713967\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.073120722905195\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.072133953970754\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.071148822466528\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.070165329951143\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.069183477986035\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.068203268135433\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.067224701966353\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.06624778104865\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.065272506954978\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.064298881260829\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.06332690554453\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.062356581387277\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.061387910373043\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.060420894088747\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.05945553412412\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.058491832071796\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.057529789527289\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.056569408088992\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.0556106893582\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.054653634939132\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.053698246438913\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.052744525467602\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.051792473638164\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.050842092566537\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.049893383871591\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.048946349175157\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.048000990102038\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.047057308279992\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.046115305339796\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.04517498291517\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.044236342642881\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.043299386162667\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.042364115117309\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.041430531152594\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.040498635917348\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.039568431063442\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.038639918245796\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.037713099122392\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.036787975354276\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.03586454860555\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.034942820543453\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.034022792838272\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.033104467163405\n",
      "Feature shape :(50000, 784)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.032187845195374\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.031272928613811\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.030359719101481\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.029448218344271\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.028538428031236\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.027630349854585\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.026723985509683\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.025819336695047\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.024916405112409\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.02401519246669\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.02311570046598\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.02221793082161\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.021321885248117\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.020427565463255\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.019534973188039\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.018644110146692\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.017754978066726\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.016867578678896\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.015981913717248\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.015097984919082\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.01421579402501\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.01333534277894\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.012456632928089\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.011579666222989\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.01070444441749\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.009830969268817\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.008959242537502\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.008089265987445\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.007221041385929\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.006354570503596\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.005489855114462\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.004626896995948\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.003765697928893\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.002906259697543\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.002048584089538\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.001192672895996\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 13.00033852791144\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.999486150933862\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.99863554376472\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.99778670820892\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.996939646074889\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.996094359174496\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.995250849323147\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.994409118339743\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.993569168046724\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.99273100027003\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.991894616839152\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.991060019587136\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.990227210350609\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.989396190969721\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.988566963288232\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.9877395291535\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.986913890416455\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.98609004893166\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.985268006557282\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.984447765155128\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.983629326590654\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.982812692732958\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.981997865454785\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.98118484663256\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.980373638146418\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.97956424188013\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.978756659721231\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.977950893560916\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.977146945294129\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.976344816819546\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.975544510039587\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.97474602686041\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.973949369191967\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.973154538947956\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.972361538045877\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.971570368407026\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.970781031956484\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.969993530623189\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.969207866339872\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.968424041043129\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.967642056673377\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.966861915174915\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.966083618495906\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.965307168588401\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.964532567408328\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.96375981691553\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.962988919073762\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.962219875850703\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.961452689217978\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.960687361151136\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.959923893629696\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.959162288637154\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.95840254816098\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.957644674192625\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 12.956888668727565\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.956134533765246\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.955382271309194\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.954631883366924\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.95388337195002\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.953136739074129\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.952391986758942\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.95164911702825\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.950908131909925\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.950169033435944\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.949431823642412\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.948696504569543\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.94796307826168\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.947231546767329\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.946501912139158\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.945774176433988\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.945048341712845\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.944324410040927\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.943602383487649\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.94288226412665\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.942164054035782\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.941447755297125\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.940733369997055\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.940020900226175\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.939310348079367\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.938601715655809\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.937895005058973\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.937190218396639\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.93648735778091\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.93578642532823\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.93508742315937\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.934390353399474\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.933695218178059\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.933002019629008\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.932310759890596\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.931621441105529\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.930934065420898\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.930248634988237\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.929565151963523\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.928883618507175\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.928204036784086\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.927526408963638\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.926850737219672\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.926177023730553\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.925505270679162\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.924835480252877\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.92416765464365\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.923501796047951\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.922837906666846\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.922175988705945\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.92151604437546\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.920858075890195\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.92020208546958\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.919548075337643\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.91889604772307\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.918246004859196\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.917597948984001\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.916951882340156\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.916307807175006\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.915665725740592\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.915025640293686\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.91438755309576\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.913751466413029\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.913117382516472\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.912485303681812\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.911855232189556\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.911227170324986\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.910601120378189\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.90997708464408\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.909355065422375\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.908735065017638\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.908117085739281\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.907501129901588\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.906887199823709\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.906275297829684\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.905665426248452\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.905057587413886\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.90445178366475\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.903848017344794\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.903246290802675\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.902646606392063\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.902048966471563\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.90145337340479\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.900859829560385\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.900268337311973\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.899678899038236\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.899091517122875\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.898506193954681\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.897922931927495\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.897341733440227\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.89676260089693\n",
      "Feature shape :(50000, 784)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.896185536706724\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.895610543283874\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.895037623047775\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.89446677842296\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.893898011839148\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.89333132573122\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.89276672253925\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.892204204708513\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.89164377468952\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.891085434937956\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.89052918791482\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.889975036086327\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.889422981923946\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.888873027904488\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.888325176509987\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.887779430227832\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.88723579155074\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.886694262976745\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.886154847009237\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.885617546156963\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.885082362934064\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.884549299860055\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.88401835945987\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.88348954426385\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.882962856807769\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.882438299632854\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.881915875285769\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.881395586318677\n",
      "Feature shape :(50000, 784)\n",
      "(784, 10)\n",
      "Feature shape :(50000, 784)\n",
      "Cost: 12.880877435289223\n",
      "Feature shape :(10000, 784)\n",
      "predictedVal :(10000, 10)\n",
      "Accuracy: 68.08\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.99      0.63       980\n",
      "          1       0.98      0.79      0.87      1135\n",
      "          2       0.89      0.69      0.78      1032\n",
      "          3       0.66      0.84      0.74      1010\n",
      "          4       0.98      0.41      0.58       982\n",
      "          5       0.00      0.00      0.00       892\n",
      "          6       0.86      0.76      0.80       958\n",
      "          7       0.97      0.73      0.83      1028\n",
      "          8       0.44      0.80      0.57       974\n",
      "          9       0.67      0.72      0.70      1009\n",
      "\n",
      "avg / total       0.70      0.68      0.66     10000\n",
      "\n",
      "Confusion matrix : \n",
      "[[967   0   1   2   0   0   1   0   9   0]\n",
      " [  0 893   1  20   0   0   5   0 216   0]\n",
      " [117   2 715  67   2   0  33   5  90   1]\n",
      " [ 66   0  14 848   0   0   2   5  72   3]\n",
      " [ 81   0  11  12 401   0  53   0 146 278]\n",
      " [426   2  11 210   0   0  11   1 219  12]\n",
      " [183   2  10   3   1   0 726   0  33   0]\n",
      " [ 70  13  26  13   1   0   2 748 102  53]\n",
      " [ 75   0   5  91   1   0  12   3 782   5]\n",
      " [100   1   9  27   3   0   4  12 125 728]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Training model for different hyperparameters RawData, RawTarget, Weights, learning rate, epochs\n",
    "#For MNIST dataset\n",
    "weights,cost=train(training_data[0],training_label,np.zeros((training_data[0].shape[1],10)),0.001,300)\n",
    "#Cost_function=cost_function(training_data[0],training_label,0)\n",
    "predictedValLogMNIST=predict(test_data[0],weights)\n",
    "\n",
    "print(\"predictedVal :\"+str(predictedValLogMNIST.shape))\n",
    "Model_accuracy=accuracy_model(predictedValLogMNIST,test_label)\n",
    "#print(\"Cost_function :\"+str(Cost_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(predictedValLogMNIST.shape)\n",
    "newList = np.array([np.argmax(t) for t in predictedValLogMNIST])\n",
    "majorityListMNIST.append(newList)\n",
    "print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape :(19999, 784)\n",
      "predictedVal :(19999, 10)\n",
      "predicted_labels :(19999, 10)\n",
      "actual_labels :(19999, 10)\n",
      "Accuracy: 22.716135806790337\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.16      0.75      0.27      2000\n",
      "          1       0.32      0.06      0.11      2000\n",
      "          2       0.32      0.38      0.34      1999\n",
      "          3       0.30      0.40      0.34      2000\n",
      "          4       0.73      0.18      0.29      2000\n",
      "          5       0.00      0.00      0.00      2000\n",
      "          6       0.42      0.12      0.19      2000\n",
      "          7       0.19      0.08      0.11      2000\n",
      "          8       0.17      0.27      0.20      2000\n",
      "          9       0.23      0.03      0.06      2000\n",
      "\n",
      "avg / total       0.29      0.23      0.19     19999\n",
      "\n",
      "Confusion matrix : \n",
      "[[1495    1  232   48   54    0   17    7   79   67]\n",
      " [ 523  128  225  259   21    0   27  289  512   16]\n",
      " [ 903    9  750   94    5    0   57   19  156    6]\n",
      " [ 902    0   56  807    1    0   18   14  182   20]\n",
      " [ 701   31   77  125  365    0   33  117  474   77]\n",
      " [1252    6  135  293    3    0   56   16  228   11]\n",
      " [1432    4  188   37   24    0  241    4   68    2]\n",
      " [ 491  128  376  353    1    0   36  155  450   10]\n",
      " [1025    5  150  171   15    0   82    7  535   10]\n",
      " [ 454   82  162  494    9    0    9  178  545   67]]\n",
      "majorityListMNIST shape: (5, 19999)\n",
      "(19999, 10)\n",
      "majorityListUSPS shape: (6, 19999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Training model for different hyperparameters RawData, RawTarget, Weights, learning rate, epochs\n",
    "#For USPS dataset\n",
    "\n",
    "predictedValLogUSPS=predict(np.array(USPSMat),weights)\n",
    "print(\"predictedVal :\"+str(predictedValLogUSPS.shape))\n",
    "Model_accuracy=accuracy_model(predictedValLogUSPS,one_hot_encode(USPSTar,10))\n",
    "#print(\"Cost_function :\"+str(Cost_function))\n",
    "\n",
    "print(predictedValLogUSPS.shape)\n",
    "newList = np.array([np.argmax(t) for t in predictedValLogUSPS])\n",
    "majorityListUSPS.append(newList)\n",
    "print(\"majorityListUSPS shape: \"+ str(np.matrix(majorityListUSPS).shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels_list, max_number):\n",
    "    \n",
    "    samples_number = len(labels_list)\n",
    "    b = np.zeros((samples_number, max_number))\n",
    "    b[np.arange(samples_number), labels_list] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(W, X, Y):\n",
    "    eta=0.01\n",
    "    m = X.shape[0]\n",
    "    Y_tilde = infer(W, X)    \n",
    "    return (-1 / m) * np.sum(np.log(Y_tilde) * Y) + eta / 2 * np.sum(W * W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, epsilon=1e-9):\n",
    "    \n",
    "    e = np.exp(x - np.max(x))\n",
    "    # print(e)\n",
    "    if e.ndim == 1:\n",
    "        return e / np.sum(e, axis=0) + epsilon\n",
    "    else:  \n",
    "        return e / np.array([np.sum(e, axis=1)]).T + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(W, X):\n",
    "    \n",
    "    X_ones = np.hstack((X, np.ones(((X.shape[0]), 1))))\n",
    "    XW = np.dot(X_ones, W)\n",
    "    smax = softmax(XW)\n",
    "    return smax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(W, X, Y):\n",
    "    \n",
    "    eta=0.01\n",
    "    X_alt = np.hstack((X, np.ones(((X.shape[0]), 1))))\n",
    "    m = X.shape[0]\n",
    "    Y_tilde = infer(W, X)   \n",
    "    return (-1 / m) * np.dot(X_alt.T, (Y - Y_tilde)) + eta * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, batch_size=50, num_epoch=1, n_classes=10, step=1e-3, plot_loss=True):\n",
    "    \n",
    "    losses = []\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    # Initialize from normal distribution\n",
    "    w = np.random.randn(n_features+1, n_classes)/n_features\n",
    "    # perform gradient descent\n",
    "    # np.seterr(divide='warn', invalid='warn')\n",
    "    for epoch in range(num_epoch):\n",
    "#         grad = get_grad(w, X_train, one_hot_encode(y_train, n_classes))\n",
    "#         # print(grad)\n",
    "#         w = w - step * grad\n",
    "#         # print(w)\n",
    "#         # print(np.isnan(np.isnan(w).sum().sum(), np.isnan(grad).sum().sum()))\n",
    "#         losses.append(loss(w, X_train, one_hot_encode(y_train, n_classes)))\n",
    "        \n",
    "        for iter_num, (x_batch, y_batch) in enumerate(zip(np.split(X_train, batch_size), np.split(y_train, batch_size))):\n",
    "            grad = get_grad(w, x_batch, one_hot_encode(y_batch, n_classes))\n",
    "            # print('W before: ', w)\n",
    "            gradient_step = step * grad\n",
    "            # print('Gradient step', grad)\n",
    "            # print(gradient_step)\n",
    "            #print('X batch', x_batch, np.isnan(x_batch).sum().sum())\n",
    "            # print(np.isnan(gradient_step).sum().sum(), np.isnan(w).sum().sum(), np.isnan(grad).sum().sum())\n",
    "            w -= gradient_step\n",
    "            # print('W after: ', w)\n",
    "            # print(w)\n",
    "            losses.append(loss(w, x_batch, one_hot_encode(y_batch, n_classes)))\n",
    "            \n",
    "    # draw learning curve \n",
    "    if plot_loss:\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.show()\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(X, W):\n",
    "    \"\"\"\n",
    "        Take X with shape [n_samples, n_features]\n",
    "        return: np.array of labels with shape [n_samples]\n",
    "    \"\"\"\n",
    "    probability_matrix = infer(W, X)\n",
    "    return np.array([np.argmax(t) for t in probability_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = train(training_data[0], training_data[1], num_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For MNIST dataset\n",
    "def accuracy(W):\n",
    "    y_pred=make_prediction(test_data[0],W)\n",
    "    right=0\n",
    "    wrong=0\n",
    "    for i,j in zip(test_data[1],y_pred):\n",
    "        if i==j:\n",
    "            right = right + 1\n",
    "        else:\n",
    "            wrong = wrong + 1\n",
    "    print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "    print(\"Classification report: \\n\"+ str(classification_report(test_data[1], y_pred)))\n",
    "    print(\"Confusion matrix : \\n\"+ str(confusion_matrix(test_data[1], y_pred)))\n",
    "   \n",
    "    newList = np.array([np.argmax(t) for t in y_pred])\n",
    "    majorityListMNIST.append(newList)\n",
    "    print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.82\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95       980\n",
      "          1       0.94      0.97      0.96      1135\n",
      "          2       0.91      0.85      0.88      1032\n",
      "          3       0.88      0.89      0.89      1010\n",
      "          4       0.89      0.91      0.90       982\n",
      "          5       0.89      0.81      0.85       892\n",
      "          6       0.92      0.93      0.92       958\n",
      "          7       0.91      0.89      0.90      1028\n",
      "          8       0.85      0.85      0.85       974\n",
      "          9       0.86      0.88      0.87      1009\n",
      "\n",
      "avg / total       0.90      0.90      0.90     10000\n",
      "\n",
      "Confusion matrix : \n",
      "[[ 957    0    2    3    0    0   10    1    7    0]\n",
      " [   0 1103    2    4    1    2    4    0   19    0]\n",
      " [  13    9  881   19   18    0   20   22   42    8]\n",
      " [   6    2   17  898    1   31    7   15   20   13]\n",
      " [   2    6    5    0  898    1   10    1    8   51]\n",
      " [  16    8    5   46   15  724   19   10   38   11]\n",
      " [  16    3    7    2   13   17  895    1    4    0]\n",
      " [   3   21   29    4   11    0    0  913    4   43]\n",
      " [   9   12   10   31   11   27   13   14  830   17]\n",
      " [  14    8    7   10   44   15    0   22    6  883]]\n",
      "majorityListMNIST shape: (2, 10000)\n"
     ]
    }
   ],
   "source": [
    "accuracy(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression Model-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#For MNIST dataset\n",
    "clf = LogisticRegression(random_state=1, solver='lbfgs',multi_class='multinomial').fit(training_data[0], training_data[1])\n",
    "LogValidationVec=clf.predict(validation_data[0])\n",
    "LogTestVec=clf.predict(test_data[0])\n",
    "clf.score(training_data[0], training_data[1])\n",
    "majorityListMNIST.append(LogTestVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majorityListMNIST shape: (3, 10000)\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.97       980\n",
      "          1       0.96      0.98      0.97      1135\n",
      "          2       0.93      0.90      0.91      1032\n",
      "          3       0.91      0.91      0.91      1010\n",
      "          4       0.94      0.93      0.93       982\n",
      "          5       0.90      0.87      0.89       892\n",
      "          6       0.93      0.95      0.94       958\n",
      "          7       0.93      0.93      0.93      1028\n",
      "          8       0.88      0.88      0.88       974\n",
      "          9       0.91      0.91      0.91      1009\n",
      "\n",
      "avg / total       0.93      0.93      0.93     10000\n",
      "\n",
      "Confusion matrix : \n",
      "[[ 958    0    0    2    1    7    6    5    1    0]\n",
      " [   0 1113    3    1    0    2    4    2   10    0]\n",
      " [   4   10  931   16    5    4   16    9   33    4]\n",
      " [   4    1   18  918    2   23    4   11   21    8]\n",
      " [   1    2    6    3  912    0    9    5    8   36]\n",
      " [  10    3    4   36    8  777   13    5   30    6]\n",
      " [   9    3    9    1    7   13  912    3    1    0]\n",
      " [   1    7   23    9    6    1    0  952    3   26]\n",
      " [   8   10    8   19    7   27   14    8  859   14]\n",
      " [  10    8    1    9   25    5    0   21    8  922]]\n"
     ]
    }
   ],
   "source": [
    "print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))\n",
    "print(\"Classification report: \\n\"+ str(classification_report(test_data[1], LogTestVec)))\n",
    "print(\"Confusion matrix : \\n\"+ str(confusion_matrix(test_data[1], LogTestVec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For USPS dataset\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(training_data[0], training_data[1])\n",
    "LogTrainingVec=clf.predict(USPSMat)\n",
    "clf.score(USPSMat, USPSTar)\n",
    "majorityListUSPS.append(LogTrainingVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majorityListUSPS shape: (1, 19999)\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.18      0.25      2000\n",
      "          1       0.69      0.13      0.22      2000\n",
      "          2       0.34      0.63      0.45      1999\n",
      "          3       0.25      0.41      0.31      2000\n",
      "          4       0.52      0.34      0.41      2000\n",
      "          5       0.28      0.59      0.38      2000\n",
      "          6       0.65      0.27      0.38      2000\n",
      "          7       0.21      0.36      0.26      2000\n",
      "          8       0.17      0.10      0.12      2000\n",
      "          9       0.28      0.10      0.15      2000\n",
      "\n",
      "avg / total       0.38      0.31      0.29     19999\n",
      "\n",
      "Confusion matrix : \n",
      "[[ 369    1  198  189   79  339   42  502   93  188]\n",
      " [  41  264  401  140  254  237   15  489  132   27]\n",
      " [  48   29 1267  117   23  336   68   42   46   23]\n",
      " [  36    6  310  821   10  679    5   68   46   19]\n",
      " [  33    7   81   57  684  203   24  633  166  112]\n",
      " [  41    6  353  210   22 1186   33   77   60   12]\n",
      " [  84    8  744   91   37  413  543   26    8   46]\n",
      " [  78   44   82  597   50  200   11  719  167   52]\n",
      " [ 179    5  146  533   78  586   84  156  194   39]\n",
      " [  14   10   95  549   76   89    6  746  215  200]]\n"
     ]
    }
   ],
   "source": [
    "print(\"majorityListUSPS shape: \"+ str(np.matrix(majorityListUSPS).shape))\n",
    "print(\"Classification report: \\n\"+ str(classification_report(USPSTar, LogTrainingVec)))\n",
    "print(\"Confusion matrix : \\n\"+ str(confusion_matrix(USPSTar, LogTrainingVec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Placeholder\n",
    "inputTensor  = tf.placeholder(tf.float32, [None, 784])\n",
    "outputTensor = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN_NEURONS_LAYER_1 = 2000\n",
    "LEARNING_RATE = 0.02\n",
    "\n",
    "# Initializing the weights to Normal Distribution\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "\n",
    "# Initializing the input to hidden layer weights\n",
    "input_hidden_weights  = init_weights([784, NUM_HIDDEN_NEURONS_LAYER_1])\n",
    "# Initializing the hidden to output layer weights\n",
    "hidden_output_weights = init_weights([NUM_HIDDEN_NEURONS_LAYER_1, 10])\n",
    "\n",
    "# Computing values at the hidden layer\n",
    "#print(\"inputTensor: \"+str(inputTensor))\n",
    "#print(\"input_hidden_weights: \"+str(input_hidden_weights))\n",
    "hidden_layer = tf.nn.relu(tf.matmul(inputTensor, input_hidden_weights))\n",
    "# Computing values at the output layer\n",
    "output_layer = tf.matmul(hidden_layer, hidden_output_weights)\n",
    "\n",
    "# Defining Error Function\n",
    "error_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=outputTensor))\n",
    "\n",
    "# Defining Learning Algorithm and Training Parameters\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n",
    "\n",
    "# Prediction Function\n",
    "prediction = output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b203dd0ef447f19cb420d57146ad74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_OF_EPOCHS = 300\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "training_accuracy = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Set Global Variables ?\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for epoch in tqdm_notebook(range(NUM_OF_EPOCHS)):\n",
    "        \n",
    "        \n",
    "        sess.run(training, feed_dict={inputTensor: training_data[0], \n",
    "                                          outputTensor: training_label})\n",
    "        # Training accuracy for an epoch\n",
    "        #training_accuracy.append(np.mean(np.argmax(training_label[0], axis=1) ==\n",
    "                             #sess.run(prediction, feed_dict={inputTensor: training_data[0],\n",
    "                                                             #outputTensor: training_label[0]})))\n",
    "        \n",
    "    predictedTestLabelMNIST = sess.run(prediction, feed_dict={inputTensor: test_data[0]})\n",
    "    predictedTestLabelUSPS = sess.run(prediction, feed_dict={inputTensor: USPSMat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels :(10000, 10)\n",
      "actual_labels :(10000, 10)\n",
      "Accuracy: 83.45\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.96      0.92       980\n",
      "          1       0.85      0.97      0.90      1135\n",
      "          2       0.87      0.80      0.83      1032\n",
      "          3       0.75      0.87      0.80      1010\n",
      "          4       0.82      0.84      0.83       982\n",
      "          5       0.90      0.55      0.68       892\n",
      "          6       0.86      0.91      0.89       958\n",
      "          7       0.87      0.86      0.87      1028\n",
      "          8       0.79      0.75      0.77       974\n",
      "          9       0.80      0.78      0.79      1009\n",
      "\n",
      "avg / total       0.84      0.83      0.83     10000\n",
      "\n",
      "Confusion matrix : \n",
      "[[ 945    0    3    4    0    5   17    1    5    0]\n",
      " [   0 1100    5    4    0    0    4    1   21    0]\n",
      " [  25   47  823   27   27    0   28   18   37    0]\n",
      " [   6    8   29  879    0   14    7   22   36    9]\n",
      " [   2   12    4    0  822    0   25    1   12  104]\n",
      " [  37   29   12  172   20  488   34   25   53   22]\n",
      " [  25    9   19    2   12   13  873    0    5    0]\n",
      " [   8   48   21    0   12    0    1  889   12   37]\n",
      " [  12   33   21   76   15   17   19   15  735   31]\n",
      " [  16   15    6   14   98    6    3   48   12  791]]\n",
      "majorityListMNIST shape: (4, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.45"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For MNIST dataset\n",
    "def accuracy_model(predicted_labels, actual_labels):\n",
    "    count=0\n",
    "    print(\"predicted_labels :\"+ str(predicted_labels.shape))\n",
    "    print(\"actual_labels :\"+ str(actual_labels.shape))\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if np.argmax(predicted_labels[i])==np.argmax(actual_labels[i]):\n",
    "            count=count+1;\n",
    "    accuracy= (count/len(predicted_labels))*100\n",
    "    NN1_MNIST_pred=np.array([np.argmax(t) for t in predicted_labels])\n",
    "    #print(pred.shape)\n",
    "    \n",
    "    print(\"Accuracy: \"+ str(accuracy))\n",
    "    print(\"Classification report: \\n\"+ str(classification_report(test_data[1], NN1_MNIST_pred)))\n",
    "    print(\"Confusion matrix : \\n\"+ str(confusion_matrix(test_data[1], NN1_MNIST_pred)))\n",
    "    majorityListMNIST.append(NN1_MNIST_pred)\n",
    "    print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))\n",
    "    return accuracy\n",
    "accuracy_model(predictedTestLabelMNIST,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels :(19999, 10)\n",
      "actual_labels :(19999, 10)\n",
      "Accuracy: 31.071553577678884\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.37      0.26      2000\n",
      "          1       0.28      0.17      0.21      2000\n",
      "          2       0.31      0.55      0.40      1999\n",
      "          3       0.40      0.54      0.46      2000\n",
      "          4       0.42      0.50      0.45      2000\n",
      "          5       0.44      0.26      0.33      2000\n",
      "          6       0.42      0.31      0.36      2000\n",
      "          7       0.19      0.16      0.17      2000\n",
      "          8       0.23      0.21      0.22      2000\n",
      "          9       0.20      0.05      0.08      2000\n",
      "\n",
      "avg / total       0.31      0.31      0.29     19999\n",
      "\n",
      "Confusion matrix : \n",
      "[[ 731    6  409   57  446   28  112   39   50  122]\n",
      " [ 339  330  200  166  233   18   46  397  251   20]\n",
      " [ 379   54 1091  103   56   26  129   86   65   10]\n",
      " [ 226    8  162 1080   47  157   44   93  135   48]\n",
      " [ 217  101   55   43  992   81   47  166  207   91]\n",
      " [ 362   29  307  297   55  529  169  101  115   36]\n",
      " [ 681   16  383   67  131   28  627   21   39    7]\n",
      " [ 253  309  362  323   69   57   56  311  227   33]\n",
      " [ 312   49  320  197  153  244  222   41  420   42]\n",
      " [ 141  261  212  334  202   36   25  405  281  103]]\n",
      "majorityListMNIST shape: (2, 19999)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31.071553577678884"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For USPS dataset\n",
    "\n",
    "def accuracy_model(predicted_labels, actual_labels):\n",
    "    count=0\n",
    "    print(\"predicted_labels :\"+ str(predicted_labels.shape))\n",
    "    print(\"actual_labels :\"+ str(actual_labels.shape))\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if np.argmax(predicted_labels[i])==np.argmax(actual_labels[i]):\n",
    "            count=count+1;\n",
    "    accuracy= (count/len(predicted_labels))*100\n",
    "    NN1_USPS_pred=np.array([np.argmax(t) for t in predicted_labels])\n",
    "    #print(pred.shape)\n",
    "    \n",
    "    print(\"Accuracy: \"+ str(accuracy))\n",
    "    print(\"Classification report: \\n\"+ str(classification_report(USPSTar, NN1_USPS_pred)))\n",
    "    print(\"Confusion matrix : \\n\"+ str(confusion_matrix(USPSTar, NN1_USPS_pred)))\n",
    "    majorityListUSPS.append(NN1_USPS_pred)\n",
    "    print(\"majorityListUSPS shape: \"+ str(np.matrix(majorityListUSPS).shape))\n",
    "    return accuracy\n",
    "accuracy_model(predictedTestLabelUSPS,one_hot_encode(USPSTar,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90226"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For MNIST dataset\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10, 10), random_state=1)\n",
    "clf.fit(training_data[0], training_data[1])\n",
    "NN_MNIST_pred=clf.predict(test_data[0])\n",
    "clf.score(training_data[0], training_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majorityListMNIST shape: (4, 10000)\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.95       980\n",
      "          1       0.95      0.97      0.96      1135\n",
      "          2       0.93      0.89      0.91      1032\n",
      "          3       0.87      0.88      0.87      1010\n",
      "          4       0.88      0.92      0.90       982\n",
      "          5       0.82      0.82      0.82       892\n",
      "          6       0.93      0.93      0.93       958\n",
      "          7       0.94      0.90      0.92      1028\n",
      "          8       0.85      0.82      0.83       974\n",
      "          9       0.88      0.87      0.88      1009\n",
      "\n",
      "avg / total       0.90      0.90      0.90     10000\n",
      "\n",
      "Confusion matrix : \n",
      "[[ 953    0    2    1    0   13    9    1    1    0]\n",
      " [   0 1097    2    5    1    2    2    2   24    0]\n",
      " [  11    8  917   21   11    9   16   12   24    3]\n",
      " [   2    7   25  889    1   40    0   18   24    4]\n",
      " [   3    2    4    0  907    3    9    3   11   40]\n",
      " [  14    1    4   61    5  733   16    4   40   14]\n",
      " [  16    5    4    1   11   18  894    1    8    0]\n",
      " [   5   15   19   14    7    1    0  930    2   35]\n",
      " [   4   16    7   24   24   63   12    4  798   22]\n",
      " [  11    3    0    9   67   17    1   12    9  880]]\n"
     ]
    }
   ],
   "source": [
    "majorityListMNIST.append(NN_MNIST_pred)\n",
    "print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))\n",
    "print(\"Classification report: \\n\"+ str(classification_report(test_data[1], NN_MNIST_pred)))\n",
    "print(\"Confusion matrix : \\n\"+ str(confusion_matrix(test_data[1], NN_MNIST_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majorityListUSPS shape: (2, 19999)\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.28      0.31      2000\n",
      "          1       0.36      0.14      0.20      2000\n",
      "          2       0.39      0.51      0.44      1999\n",
      "          3       0.32      0.53      0.40      2000\n",
      "          4       0.52      0.42      0.46      2000\n",
      "          5       0.27      0.47      0.35      2000\n",
      "          6       0.50      0.39      0.44      2000\n",
      "          7       0.26      0.34      0.29      2000\n",
      "          8       0.26      0.12      0.16      2000\n",
      "          9       0.21      0.17      0.19      2000\n",
      "\n",
      "avg / total       0.34      0.34      0.32     19999\n",
      "\n",
      "Confusion matrix : \n",
      "[[ 558    5  123  100  131  260  101  143   68  511]\n",
      " [  55  282  229   79  293  259   92  562   85   64]\n",
      " [ 141   36 1013  233   42  228  168   76   39   23]\n",
      " [  44   20  242 1060    5  512   14   65   26   12]\n",
      " [  25   36   37   28  841  157  105  372   92  307]\n",
      " [ 148   51  187  338   26  943  129   59   54   65]\n",
      " [ 399    8  385  133   50  177  772   16   25   35]\n",
      " [  60  173  115  497   18  225   18  674   96  124]\n",
      " [ 103   40  183  430   61  563  123  147  235  115]\n",
      " [  31  134   91  390  163  141   14  518  179  339]]\n"
     ]
    }
   ],
   "source": [
    "#For USPS dataset\n",
    "clf = MLPClassifier(random_state=0, solver='lbfgs',alpha=1e-5, hidden_layer_sizes=(10, 10)).fit(training_data[0], training_data[1])\n",
    "NN_USPS_predict=clf.predict(USPSMat)\n",
    "clf.score(USPSMat, USPSTar)\n",
    "majorityListUSPS.append(NN_USPS_predict)\n",
    "print(\"majorityListUSPS shape: \"+ str(np.matrix(majorityListUSPS).shape))\n",
    "print(\"Classification report: \\n\"+ str(classification_report(USPSTar, NN_USPS_predict)))\n",
    "print(\"Confusion matrix : \\n\"+ str(confusion_matrix(USPSTar, NN_USPS_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier Model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.47\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97       980\n",
      "          1       0.98      0.99      0.98      1135\n",
      "          2       0.93      0.95      0.94      1032\n",
      "          3       0.91      0.92      0.92      1010\n",
      "          4       0.95      0.95      0.95       982\n",
      "          5       0.93      0.92      0.93       892\n",
      "          6       0.96      0.96      0.96       958\n",
      "          7       0.96      0.94      0.95      1028\n",
      "          8       0.94      0.90      0.92       974\n",
      "          9       0.94      0.92      0.93      1009\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 961    0    3    0    1    5    3    1    5    1]\n",
      " [   1 1121    2    4    0    2    2    1    1    1]\n",
      " [  10    2  985    6    1    1    7   10    9    1]\n",
      " [   1    0   22  934    0   22    2   10   15    4]\n",
      " [   0    3    7    1  928    2    7    3    5   26]\n",
      " [   5    1    5   37    5  817    8    1   11    2]\n",
      " [  11    3    2    1    8    8  921    0    2    2]\n",
      " [   0    9   22    5    5    1    0  971    4   11]\n",
      " [   7    4    8   25   12   10   10    8  878   12]\n",
      " [   8    6    7   18   18    6    0    8    7  931]]\n",
      "majorityListMNIST shape: (6, 10000)\n"
     ]
    }
   ],
   "source": [
    "#For MNIST dataset\n",
    "classifier2 = RandomForestClassifier(n_estimators=10);\n",
    "classifier2.fit(training_data[0], training_data[1]) \n",
    "rfc_pred_MNIST=classifier2.predict(test_data[0])\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test_data[1],rfc_pred_MNIST):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(test_data[1], rfc_pred_MNIST)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(test_data[1], rfc_pred_MNIST)))\n",
    "majorityListMNIST.append(rfc_pred_MNIST)\n",
    "print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 31.82159107955398\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.29      0.31      2000\n",
      "          1       0.23      0.30      0.26      2000\n",
      "          2       0.29      0.42      0.35      1999\n",
      "          3       0.39      0.50      0.44      2000\n",
      "          4       0.35      0.46      0.40      2000\n",
      "          5       0.32      0.46      0.38      2000\n",
      "          6       0.53      0.25      0.34      2000\n",
      "          7       0.19      0.26      0.22      2000\n",
      "          8       0.33      0.07      0.12      2000\n",
      "          9       0.22      0.06      0.09      2000\n",
      "\n",
      "avg / total       0.32      0.31      0.29     19999\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 586   74  300   86  391  160   96  129   21  157]\n",
      " [  61  602  130  127  185   69   24  775   18    9]\n",
      " [ 217  192  831  122  117  165   78  225   25   27]\n",
      " [  86   80  179 1004   91  324   19  165   23   29]\n",
      " [  27  291  131  103  919  112   34  295   39   49]\n",
      " [ 195  109  185  237  112  911   60  118   29   44]\n",
      " [ 386  129  282   84  178  287  505   96   27   26]\n",
      " [  77  541  297  241  105  148   22  521   17   31]\n",
      " [ 131  215  252  279  206  556   91   78  140   52]\n",
      " [  72  355  231  276  334  117   26  387   83  119]]\n",
      "majorityListUSPS shape: (4, 19999)\n"
     ]
    }
   ],
   "source": [
    "#For USPS dataset\n",
    "classifier2 = RandomForestClassifier(n_estimators=10);\n",
    "classifier2.fit(training_data[0], training_data[1]) \n",
    "rfc_pred_USPS=classifier2.predict(USPSMat)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(USPSTar,rfc_pred_USPS):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(USPSTar, rfc_pred_USPS)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(USPSTar, rfc_pred_USPS)))\n",
    "majorityListUSPS.append(rfc_pred_USPS)\n",
    "print(\"majorityListUSPS shape: \"+ str(np.matrix(majorityListUSPS).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.82\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.96      0.97      0.97      1032\n",
      "          3       0.95      0.96      0.96      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.97      0.96      0.96       892\n",
      "          6       0.97      0.98      0.98       958\n",
      "          7       0.97      0.97      0.97      1028\n",
      "          8       0.96      0.95      0.95       974\n",
      "          9       0.96      0.95      0.95      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 969    0    0    0    0    1    4    1    4    1]\n",
      " [   0 1118    3    3    1    2    3    0    4    1]\n",
      " [   6    1  999    5    2    0    4    9    6    0]\n",
      " [   0    0    9  973    0    8    0    9    7    4]\n",
      " [   1    0    2    0  957    0    4    0    2   16]\n",
      " [   4    0    0   15    3  852    6    2    6    4]\n",
      " [   6    3    1    0    2    5  938    0    3    0]\n",
      " [   1    3   17    2    3    0    0  994    1    7]\n",
      " [   4    0    5   14    3    6    3    5  926    8]\n",
      " [   7    6    1    9   12    3    1    4   10  956]]\n",
      "majorityListMNIST shape: (5, 10000)\n"
     ]
    }
   ],
   "source": [
    "#For MNIST dataset\n",
    "classifier2 = RandomForestClassifier(n_estimators=100);\n",
    "classifier2.fit(training_data[0], training_data[1]) \n",
    "rfc2_pred_MNIST=classifier2.predict(test_data[0])\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test_data[1],rfc2_pred_MNIST):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(test_data[1], rfc2_pred_MNIST)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(test_data[1], rfc2_pred_MNIST)))\n",
    "majorityListMNIST.append(rfc2_pred_MNIST)\n",
    "print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 39.171958597929894\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.32      0.38      2000\n",
      "          1       0.36      0.27      0.31      2000\n",
      "          2       0.42      0.62      0.50      1999\n",
      "          3       0.52      0.63      0.57      2000\n",
      "          4       0.48      0.54      0.50      2000\n",
      "          5       0.33      0.70      0.44      2000\n",
      "          6       0.74      0.37      0.50      2000\n",
      "          7       0.20      0.34      0.25      2000\n",
      "          8       0.52      0.08      0.13      2000\n",
      "          9       0.24      0.05      0.09      2000\n",
      "\n",
      "avg / total       0.43      0.39      0.37     19999\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 649    9  257   63  459  154   72   98    1  238]\n",
      " [  31  545  119  117   66  113   23  966   16    4]\n",
      " [  89   30 1242   82   57  222   18  250    7    2]\n",
      " [  39    9  106 1258   59  345    3  152    3   26]\n",
      " [   6  202   67   23 1070  209   21  365   21   16]\n",
      " [ 148   23  132  108   34 1393   27  118   10    7]\n",
      " [ 340   56  230   34   93  377  745  111    6    8]\n",
      " [  33  323  391  215   43  286   30  674    1    4]\n",
      " [  48   53  185  224  119 1032   64  100  154   21]\n",
      " [  20  257  240  314  250  139    6  593   77  104]]\n",
      "majorityListUSPS shape: (3, 19999)\n"
     ]
    }
   ],
   "source": [
    "#For USPS dataset\n",
    "classifier2 = RandomForestClassifier(n_estimators=100);\n",
    "classifier2.fit(training_data[0], training_data[1]) \n",
    "rfc2_pred_USPS=classifier2.predict(USPSMat)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(USPSTar,rfc2_pred_USPS):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(USPSTar, rfc2_pred_USPS)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(USPSTar, rfc2_pred_USPS)))\n",
    "majorityListUSPS.append(rfc2_pred_USPS)\n",
    "print(\"majorityListUSPS shape: \"+ str(np.matrix(majorityListUSPS).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.05\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.96      0.97      0.97      1032\n",
      "          3       0.96      0.97      0.96      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.97      0.97      0.97       892\n",
      "          6       0.98      0.98      0.98       958\n",
      "          7       0.97      0.96      0.97      1028\n",
      "          8       0.96      0.96      0.96       974\n",
      "          9       0.96      0.95      0.95      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 969    0    1    0    0    3    3    1    3    0]\n",
      " [   0 1122    3    3    0    2    2    0    2    1]\n",
      " [   6    0  999    5    3    0    4    9    6    0]\n",
      " [   0    0   10  975    0    6    0    9    7    3]\n",
      " [   1    0    0    0  956    0    5    0    3   17]\n",
      " [   2    0    0   11    3  861    5    2    5    3]\n",
      " [   6    3    0    0    3    3  939    0    4    0]\n",
      " [   1    2   18    1    1    0    0  992    2   11]\n",
      " [   6    0    5    7    3    5    3    4  931   10]\n",
      " [   6    5    1   10   12    4    1    5    4  961]]\n",
      "majorityListMNIST shape: (6, 10000)\n"
     ]
    }
   ],
   "source": [
    "#For MNIST dataset\n",
    "classifier2 = RandomForestClassifier(n_estimators=1000);\n",
    "classifier2.fit(training_data[0], training_data[1]) \n",
    "rfc3_pred_MNIST=classifier2.predict(test_data[0])\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test_data[1],rfc3_pred_MNIST):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(test_data[1], rfc3_pred_MNIST)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(test_data[1], rfc3_pred_MNIST)))\n",
    "majorityListMNIST.append(rfc3_pred_MNIST)\n",
    "print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.8270413520676\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.33      0.39      2000\n",
      "          1       0.38      0.29      0.33      2000\n",
      "          2       0.44      0.64      0.52      1999\n",
      "          3       0.55      0.65      0.59      2000\n",
      "          4       0.50      0.54      0.52      2000\n",
      "          5       0.35      0.74      0.47      2000\n",
      "          6       0.79      0.42      0.55      2000\n",
      "          7       0.19      0.35      0.25      2000\n",
      "          8       0.55      0.08      0.14      2000\n",
      "          9       0.24      0.05      0.08      2000\n",
      "\n",
      "avg / total       0.45      0.41      0.38     19999\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 650   12  279   53  447  158   56   96    2  247]\n",
      " [  45  576  111  107   50   93   16  988   13    1]\n",
      " [  90   29 1289   71   46  190   15  262    5    2]\n",
      " [  38    7   93 1300   47  308    3  186    3   15]\n",
      " [  11  203   53   26 1076  182   12  398   19   20]\n",
      " [ 139   29  127   69   22 1473   15  115    7    4]\n",
      " [ 308   50  230   21   83  325  838  135    1    9]\n",
      " [  37  319  382  232   31  258   32  699    2    8]\n",
      " [  33   39  150  194   98 1146   67   97  163   13]\n",
      " [  19  263  222  309  238  127    7  632   82  101]]\n",
      "majorityListUSPS shape: (4, 19999)\n"
     ]
    }
   ],
   "source": [
    "#For USPS dataset\n",
    "classifier2 = RandomForestClassifier(n_estimators=1000);\n",
    "classifier2.fit(training_data[0], training_data[1]) \n",
    "rfc3_pred_USPS=classifier2.predict(USPSMat)\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(USPSTar,rfc3_pred_USPS):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(USPSTar, rfc3_pred_USPS)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(USPSTar, rfc3_pred_USPS)))\n",
    "majorityListUSPS.append(rfc3_pred_USPS)\n",
    "print(\"majorityListUSPS shape: \"+ str(np.matrix(majorityListUSPS).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-Model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For MNIST dataset\n",
    "classifier1 = SVC(kernel='rbf', C=1);\n",
    "classifier1.fit(training_data[0], training_data[1])\n",
    "svm_pred_MNIST=classifier1.predict(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.35\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97       980\n",
      "          1       0.97      0.99      0.98      1135\n",
      "          2       0.94      0.93      0.94      1032\n",
      "          3       0.93      0.94      0.93      1010\n",
      "          4       0.93      0.95      0.94       982\n",
      "          5       0.93      0.91      0.92       892\n",
      "          6       0.95      0.96      0.96       958\n",
      "          7       0.95      0.93      0.94      1028\n",
      "          8       0.94      0.91      0.93       974\n",
      "          9       0.94      0.91      0.93      1009\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 967    0    1    0    0    5    4    1    2    0]\n",
      " [   0 1120    2    3    0    1    3    1    5    0]\n",
      " [   9    1  962    7   10    1   13   11   16    2]\n",
      " [   1    1   14  950    1   17    1   10   11    4]\n",
      " [   1    1    7    0  937    0    7    2    2   25]\n",
      " [   7    4    5   33    7  808   11    2   10    5]\n",
      " [  10    3    4    1    5   10  924    0    1    0]\n",
      " [   2   13   22    5    7    1    0  954    4   20]\n",
      " [   4    6    6   14    8   24   10    8  891    3]\n",
      " [  10    6    0   12   33    5    1   14    6  922]]\n",
      "majorityListMNIST shape: (7, 10000)\n"
     ]
    }
   ],
   "source": [
    "#SVM pred for MNIST dataset\n",
    "\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test_data[1],svm_pred_MNIST):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(test_data[1], svm_pred_MNIST)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(test_data[1], svm_pred_MNIST)))\n",
    "majorityListMNIST.append(svm_pred_MNIST)\n",
    "print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For USPS dataset\n",
    "classifier1 = SVC(kernel='rbf', C=1);\n",
    "classifier1.fit(training_data[0], training_data[1])\n",
    "svm_pred_USPS=classifier1.predict(USPSMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 38.54192709635482\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.29      0.34      2000\n",
      "          1       0.45      0.21      0.29      2000\n",
      "          2       0.35      0.70      0.46      1999\n",
      "          3       0.51      0.56      0.53      2000\n",
      "          4       0.52      0.58      0.55      2000\n",
      "          5       0.29      0.68      0.41      2000\n",
      "          6       0.66      0.37      0.48      2000\n",
      "          7       0.24      0.23      0.23      2000\n",
      "          8       0.37      0.12      0.18      2000\n",
      "          9       0.27      0.10      0.15      2000\n",
      "\n",
      "avg / total       0.41      0.39      0.36     19999\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 573    2  428   19  285  248   73   44    6  322]\n",
      " [ 110  429  285  137  273  180   46  501   22   17]\n",
      " [ 128   18 1402   59   39  198   61   57   23   14]\n",
      " [  76    3  186 1123   11  483    5   70   27   16]\n",
      " [  18   67   91   14 1167  267   22  194   69   91]\n",
      " [ 108   17  257  102   25 1367   60   43   15    6]\n",
      " [ 197    7  489   24   98  394  748   13    7   23]\n",
      " [  50  225  457  265   57  416   15  452   41   22]\n",
      " [  73   25  209  193   87 1006   95   41  244   27]\n",
      " [  26  166  228  278  213  165    8  499  214  203]]\n",
      "majorityListUSPS shape: (7, 19999)\n"
     ]
    }
   ],
   "source": [
    "#SVM pred for USPS dataset\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(USPSTar,svm_pred_USPS):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(USPSTar, svm_pred_USPS)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(USPSTar, svm_pred_USPS)))\n",
    "majorityListUSPS.append(svm_pred_USPS)\n",
    "print(\"majorityListUSPS shape: \"+ str(np.matrix(majorityListUSPS).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For MNIST dataset\n",
    "classifier1 = SVC(kernel='rbf', C=1, gamma=1);\n",
    "classifier1.fit(training_data[0], training_data[1])\n",
    "svm2_pred_MNIST=classifier1.predict(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM pred for MNIST dataset\n",
    "\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test_data[1],svm2_pred_MNIST):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(test_data[1], svm2_pred_MNIST)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(test_data[1], svm2_pred_MNIST)))\n",
    "majorityListMNIST.append(svm2_pred_MNIST)\n",
    "print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For MNIST dataset\n",
    "classifier1 = SVC(kernel='linear');\n",
    "classifier1.fit(training_data[0], training_data[1])\n",
    "svm3_pred_MNIST=classifier1.predict(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.89999999999999\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       980\n",
      "          1       0.97      0.99      0.98      1135\n",
      "          2       0.92      0.94      0.93      1032\n",
      "          3       0.90      0.93      0.92      1010\n",
      "          4       0.93      0.96      0.95       982\n",
      "          5       0.92      0.89      0.91       892\n",
      "          6       0.96      0.95      0.95       958\n",
      "          7       0.95      0.93      0.94      1028\n",
      "          8       0.93      0.89      0.91       974\n",
      "          9       0.95      0.91      0.93      1009\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10000\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 959    0    5    2    2    4    7    0    1    0]\n",
      " [   0 1121    3    3    0    1    2    1    4    0]\n",
      " [   6    8  968    9    3    2   11   10   13    2]\n",
      " [   5    2   17  944    4   13    1    8   13    3]\n",
      " [   2    1   10    1  943    0    4    2    2   17]\n",
      " [  13    4    2   39    5  792    9    1   22    5]\n",
      " [  10    3   11    1    5   14  911    2    1    0]\n",
      " [   1    8   20   10    6    1    0  961    3   18]\n",
      " [   8    4    9   25   11   27    6    5  871    8]\n",
      " [   7    6    2   13   32    4    0   18    7  920]]\n",
      "majorityListMNIST shape: (11, 10000)\n"
     ]
    }
   ],
   "source": [
    "#SVM pred for MNIST dataset\n",
    "\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(test_data[1],svm3_pred_MNIST):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(test_data[1], svm3_pred_MNIST)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(test_data[1], svm3_pred_MNIST)))\n",
    "majorityListMNIST.append(svm3_pred_MNIST)\n",
    "print(\"majorityListMNIST shape: \"+ str(np.matrix(majorityListMNIST).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For USPS dataset\n",
    "classifier1 = SVC(kernel='linear');\n",
    "classifier1.fit(training_data[0], training_data[1])\n",
    "svm3_pred_USPS=classifier1.predict(USPSMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 29.12645632281614\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.17      0.24      2000\n",
      "          1       0.49      0.15      0.23      2000\n",
      "          2       0.25      0.65      0.36      1999\n",
      "          3       0.25      0.45      0.32      2000\n",
      "          4       0.46      0.40      0.43      2000\n",
      "          5       0.24      0.44      0.31      2000\n",
      "          6       0.61      0.23      0.33      2000\n",
      "          7       0.23      0.26      0.24      2000\n",
      "          8       0.25      0.08      0.12      2000\n",
      "          9       0.28      0.08      0.13      2000\n",
      "\n",
      "avg / total       0.34      0.29      0.27     19999\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 348    0  476  152  222  345   74  172   10  201]\n",
      " [  60  303  534  275  230  172   17  351   37   21]\n",
      " [ 139   63 1293  115   33  221   55   45   21   14]\n",
      " [  56   58  341  898    8  520    9   45   48   17]\n",
      " [  24   24  221   82  800  215   10  464   82   78]\n",
      " [  47   25  652  240   41  876   30   35   41   13]\n",
      " [ 146   19  903   55   86  264  462   38    2   25]\n",
      " [  19   74  201  706   54  294   12  522   84   34]\n",
      " [ 100   16  298  449  126  692   82   58  160   19]\n",
      " [  18   38  204  588  142  104    8  580  155  163]]\n",
      "majorityListUSPS shape: (8, 19999)\n"
     ]
    }
   ],
   "source": [
    "#SVM pred for USPS dataset\n",
    "right=0\n",
    "wrong=0\n",
    "for i,j in zip(USPSTar,svm3_pred_USPS):\n",
    "    if i==j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "print(\"Classification report: \\n\"+ str(classification_report(USPSTar, svm3_pred_USPS)))\n",
    "print(\"Confusion matrix: \\n\"+str(confusion_matrix(USPSTar, svm3_pred_USPS)))\n",
    "majorityListUSPS.append(svm3_pred_USPS)\n",
    "print(\"majorityListUSPS shape: \"+ str(np.matrix(majorityListUSPS).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"SVM3MNIST.txt\",svm3_pred_MNIST,fmt='%i')\n",
    "np.savetxt(\"SVM3USPS.txt\",svm3_pred_USPS,fmt='%i')\n",
    "np.savetxt(\"SVM1USPS.txt\",svm_pred_USPS,fmt='%i')\n",
    "np.savetxt(\"SVM1MNIST.txt\",svm_pred_MNIST,fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMajority(majorityListMNIST):\n",
    "    m=stats.mode(np.array(majorityListMNIST))\n",
    "    right=0\n",
    "    wrong=0\n",
    "    for i,j in zip(test_data[1],m[0][0]):\n",
    "        if i==j:\n",
    "            right = right + 1\n",
    "        else:\n",
    "            wrong = wrong + 1\n",
    "    print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "    print(\"Classification report: \\n\"+ str(classification_report(test_data[1], m[0][0])))\n",
    "    print(\"Confusion matrix: \\n\"+str(confusion_matrix(test_data[1], m[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.42\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.99      0.94       980\n",
      "          1       0.97      0.99      0.98      1135\n",
      "          2       0.94      0.92      0.93      1032\n",
      "          3       0.89      0.94      0.92      1010\n",
      "          4       0.94      0.95      0.94       982\n",
      "          5       0.95      0.86      0.90       892\n",
      "          6       0.95      0.96      0.95       958\n",
      "          7       0.94      0.94      0.94      1028\n",
      "          8       0.93      0.89      0.91       974\n",
      "          9       0.95      0.91      0.93      1009\n",
      "\n",
      "avg / total       0.93      0.93      0.93     10000\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 972    0    1    2    0    1    2    1    1    0]\n",
      " [   0 1121    2    2    0    1    4    1    4    0]\n",
      " [  21    6  946    9    6    0   14   12   18    0]\n",
      " [   7    0   20  946    0    8    0   13   14    2]\n",
      " [   4    1    4    0  930    0   10    2    6   25]\n",
      " [  23    2    3   56    6  766   10    4   19    3]\n",
      " [  22    3    3    1    5    6  917    0    1    0]\n",
      " [   5   11   20    6    3    0    0  963    3   17]\n",
      " [  19    7    6   20   11   18   13    9  865    6]\n",
      " [  19    7    1   15   27    3    0   17    4  916]]\n"
     ]
    }
   ],
   "source": [
    "findMajority(majorityListMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR USPS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMajority(majorityListUSPS):\n",
    "    m=stats.mode(np.array(majorityListUSPS))\n",
    "    right=0\n",
    "    wrong=0\n",
    "    for i,j in zip(USPSTar,m[0][0]):\n",
    "        if i==j:\n",
    "            right = right + 1\n",
    "        else:\n",
    "            wrong = wrong + 1\n",
    "    print(\"Accuracy: \" + str(right/(right+wrong)*100)) \n",
    "    print(\"Classification report: \\n\"+ str(classification_report(USPSTar, m[0][0])))\n",
    "    print(\"Confusion matrix: \\n\"+str(confusion_matrix(USPSTar, m[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 31.046552327616382\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.68      0.30      2000\n",
      "          1       0.38      0.16      0.22      2000\n",
      "          2       0.36      0.52      0.42      1999\n",
      "          3       0.36      0.51      0.42      2000\n",
      "          4       0.63      0.38      0.47      2000\n",
      "          5       0.48      0.26      0.34      2000\n",
      "          6       0.56      0.20      0.30      2000\n",
      "          7       0.24      0.16      0.19      2000\n",
      "          8       0.21      0.20      0.20      2000\n",
      "          9       0.26      0.03      0.06      2000\n",
      "\n",
      "avg / total       0.37      0.31      0.29     19999\n",
      "\n",
      "Confusion matrix: \n",
      "[[1355    2  242   53  136   24   20   21   43  104]\n",
      " [ 441  321  251  183  114   38   24  363  263    2]\n",
      " [ 641   34 1041   76   14   24   62   42   59    6]\n",
      " [ 570    4  115 1024    5  131   17   20  102   12]\n",
      " [ 436   65   70   95  759   45   21  183  280   46]\n",
      " [ 801   17  183  289   11  524   52   27   93    3]\n",
      " [1160    7  280   40   43   22  407    4   36    1]\n",
      " [ 385  221  361  368   12   38   30  322  260    3]\n",
      " [ 760   21  184  238   43  228   90   34  391   11]\n",
      " [ 339  163  179  456   63   19    6  350  360   65]]\n"
     ]
    }
   ],
   "source": [
    "findMajority(majorityListUSPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
